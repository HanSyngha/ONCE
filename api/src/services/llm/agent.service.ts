/**
 * LLM Agent Service
 *
 * LLM ê¸°ë°˜ Agentic ë…¸íŠ¸ ì‹œìŠ¤í…œì˜ í•µì‹¬ ë¡œì§
 * - Tool call ê¸°ë°˜ recursive ë™ì‘
 * - History ìœ ì§€ (ë§¤ iterationë§ˆë‹¤)
 * - í† í° ê´€ë¦¬ (80% ì„ê³„ì¹˜)
 */

import { prisma, io, redis } from '../../index.js';
import { executeTool, getToolDefinitions, ToolResult } from './tools.service.js';
import { updateTokenUsage, getTokenWarning, TokenUsageStatus, createAgentSession } from './token.service.js';
import { emitRequestProgress } from '../../websocket/server.js';
import { sendFailureEmail } from '../mail.service.js';

// LLM Proxy ì„¤ì •
const LLM_PROXY_URL = process.env.LLM_PROXY_URL || 'http://localhost:3400/api/v1';
const LLM_SERVICE_ID = process.env.LLM_SERVICE_ID || 'aipo-web';
const MODEL_CONFIG_KEY = 'aipo:model_config';

interface ModelConfig {
  defaultModel: string;
  fallbackModels: string[];
}

/**
 * Dashboard /v1/models APIì—ì„œ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ
 */
async function fetchFirstAvailableModel(): Promise<string | null> {
  try {
    const baseUrl = LLM_PROXY_URL
      .replace(/\/chat\/completions$/, '')
      .replace(/\/v1$/, '');
    const modelsUrl = `${baseUrl}/v1/models`;
    const response = await fetch(modelsUrl, {
      headers: {
        'Content-Type': 'application/json',
        'X-Service-Id': LLM_SERVICE_ID,
      },
    });
    if (response.ok) {
      const data = await response.json() as any;
      const models = data.data || [];
      if (models.length > 0) {
        return models[0].id;
      }
    }
  } catch (e) {
    console.error('[Agent] Failed to fetch models from proxy:', e);
  }
  return null;
}

/**
 * Redisì—ì„œ ëª¨ë¸ ì„¤ì • ì¡°íšŒ. ì—†ìœ¼ë©´ Dashboard APIì—ì„œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜´
 */
async function getModelConfig(): Promise<ModelConfig> {
  try {
    const configStr = await redis.get(MODEL_CONFIG_KEY);
    if (configStr) {
      const config = JSON.parse(configStr);
      if (config.defaultModel) {
        return {
          defaultModel: config.defaultModel,
          fallbackModels: config.fallbackModels || [],
        };
      }
    }
  } catch (e) {
    console.error('[Agent] Failed to read model config from Redis:', e);
  }

  // Redisì— ì„¤ì •ì´ ì—†ìœ¼ë©´ Dashboard APIì—ì„œ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
  const firstModel = await fetchFirstAvailableModel();
  if (firstModel) {
    return { defaultModel: firstModel, fallbackModels: [] };
  }

  // ìµœí›„ ìˆ˜ë‹¨: í™˜ê²½ë³€ìˆ˜ (ì„¤ì • ì•ˆ ë˜ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë°œìƒí•˜ê²Œ ë¹ˆ ë¬¸ìì—´)
  const envModel = process.env.LLM_DEFAULT_MODEL || '';
  if (!envModel) {
    console.error('[Agent] No model available: Redis empty, API unreachable, LLM_DEFAULT_MODEL not set');
  }
  return { defaultModel: envModel, fallbackModels: [] };
}

// ì œí•œ ì„¤ì •
const MAX_ITERATIONS = 100;

interface LLMMessage {
  role: 'system' | 'user' | 'assistant' | 'tool';
  content?: string | null;
  tool_calls?: ToolCall[];
  tool_call_id?: string;
  name?: string;
}

interface ToolCall {
  id: string;
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
}

interface LLMResponse {
  choices: Array<{
    message: {
      role: string;
      content?: string;
      tool_calls?: ToolCall[];
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

interface AgentResult {
  filesCreated: string[];
  filesModified: string[];
  foldersCreated: string[];
  summary?: string;
  searchResults?: Array<{
    fileId: string;
    path: string;
    title: string;
    snippet: string;
    relevanceScore: number;
  }>;
}

/**
 * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
 */
function getSystemPrompt(type: 'INPUT' | 'SEARCH' | 'REFACTOR', treeStructure: string): string {
  const basePrompt = `ë‹¹ì‹ ì€ AIPO for Webì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë…¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

## í˜„ì¬ í´ë”/íŒŒì¼ êµ¬ì¡°
${treeStructure || '(ë¹ˆ ê³µê°„)'}

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
`;

  if (type === 'INPUT') {
    return basePrompt + `
### í´ë” ê´€ë ¨
- add_folder(path): ìƒˆ í´ë” ìƒì„± (ì˜ˆ: /projects/aipo)
- undo_add_folder(path): í´ë” ìƒì„± ì·¨ì†Œ
- edit_folder_name(path, newName): í´ë” ì´ë¦„ ë³€ê²½

### íŒŒì¼ ê´€ë ¨
- add_file(path, content): ìƒˆ íŒŒì¼ ìƒì„± (contentëŠ” BlockNote JSON í˜•ì‹)
- undo_add_file(path): íŒŒì¼ ìƒì„± ì·¨ì†Œ
- read_file(path): íŒŒì¼ ë‚´ìš© ì½ê¸°
- edit_file(path, before, after): íŒŒì¼ ë‚´ìš© ìˆ˜ì • (beforeê°€ í˜„ì¬ ë‚´ìš©ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
- edit_file_name(path, newName): íŒŒì¼ ì´ë¦„ ë³€ê²½
- move_file(fromPath, toPath): íŒŒì¼ ì´ë™

### ì™„ë£Œ
- complete(summary): ì‘ì—… ì™„ë£Œ ì„ ì–¸

## ê·œì¹™
1. ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í´ë” êµ¬ì¡°ì™€ íŒŒì¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
2. ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€í• ì§€, ìƒˆ íŒŒì¼ì„ ë§Œë“¤ì§€ íŒë‹¨í•˜ì„¸ìš”.
3. ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
4. ì‘ì—…ì´ ì™„ë£Œë˜ë©´ ë°˜ë“œì‹œ complete()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
5. í•œêµ­ì–´ë¡œ ë…¸íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## ì½˜í…ì¸  í˜•ì‹
íŒŒì¼ contentëŠ” BlockNote JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤:
[
  { "type": "heading", "props": { "level": 1 }, "content": [{ "type": "text", "text": "ì œëª©" }] },
  { "type": "paragraph", "content": [{ "type": "text", "text": "ë‚´ìš©..." }] }
]
`;
  }

  if (type === 'SEARCH') {
    return basePrompt + `
### ê²€ìƒ‰ ë„êµ¬
- read_file(path): íŒŒì¼ ë‚´ìš© ì½ê¸°
- complete(summary, searchResults): ê²€ìƒ‰ ì™„ë£Œ

## ê·œì¹™
1. ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
2. íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë³´ê³  ê´€ë ¨ ìˆì–´ ë³´ì´ëŠ” íŒŒì¼ì„ read_fileë¡œ í™•ì¸í•˜ì„¸ìš”.
3. ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.
4. searchResults ë°°ì—´ì— ê²°ê³¼ë¥¼ ë‹´ì•„ complete()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

## searchResults í˜•ì‹
[
  { "fileId": "...", "path": "...", "title": "...", "snippet": "ê´€ë ¨ ë‚´ìš©...", "relevanceScore": 95 }
]
`;
  }

  // REFACTOR
  return basePrompt + `
### ë¦¬íŒ©í† ë§ ë„êµ¬
- add_folder(path): ìƒˆ í´ë” ìƒì„±
- add_file(path, content): ìƒˆ íŒŒì¼ ìƒì„±
- read_file(path): íŒŒì¼ ë‚´ìš© ì½ê¸°
- edit_file(path, before, after): íŒŒì¼ ë‚´ìš© ìˆ˜ì •
- move_file(fromPath, toPath): íŒŒì¼ ì´ë™
- delete_file(path): íŒŒì¼ ì‚­ì œ (íœ´ì§€í†µìœ¼ë¡œ)
- delete_folder(path): ë¹ˆ í´ë” ì‚­ì œ
- complete(summary): ì‘ì—… ì™„ë£Œ

## ê·œì¹™
1. í˜„ì¬ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ìµœì ì˜ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì„¸ìš”.
2. ë‚´ìš©ì´ ìœ ì‹¤ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
3. ë¹„ìŠ·í•œ ì£¼ì œì˜ ë…¸íŠ¸ë¥¼ ê°™ì€ í´ë”ì— ëª¨ìœ¼ì„¸ìš”.
4. ì‘ì—… ì™„ë£Œ í›„ ë³€ê²½ ë‚´ìš©ì„ summaryì— ìš”ì•½í•˜ì„¸ìš”.
`;
}

/**
 * LLM API í˜¸ì¶œ (ë‹¨ì¼ ëª¨ë¸)
 */
async function callLLMWithModel(
  messages: LLMMessage[],
  user: { loginid: string; username: string; deptname: string },
  model: string
): Promise<LLMResponse> {
  const response = await fetch(LLM_PROXY_URL, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'X-User-Id': user.loginid,
      'X-User-Name': encodeURIComponent(user.username),
      'X-User-Dept': encodeURIComponent(user.deptname),
      'X-Service-Id': LLM_SERVICE_ID,
    },
    body: JSON.stringify({
      model,
      messages,
      tools: getToolDefinitions(),
      tool_choice: 'auto',
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`LLM API error (model: ${model}): ${response.status} - ${error}`);
  }

  return response.json() as Promise<LLMResponse>;
}

/**
 * LLM API í˜¸ì¶œ (default â†’ fallback ìˆœì„œë¡œ ì‹œë„)
 */
async function callLLM(
  messages: LLMMessage[],
  user: { loginid: string; username: string; deptname: string }
): Promise<LLMResponse> {
  const config = await getModelConfig();
  const modelsToTry = [config.defaultModel, ...config.fallbackModels];

  let lastError: Error | null = null;

  for (const model of modelsToTry) {
    try {
      console.log(`[Agent] Trying model: ${model}`);
      const response = await callLLMWithModel(messages, user, model);
      return response;
    } catch (error) {
      lastError = error as Error;
      console.warn(`[Agent] Model ${model} failed:`, (error as Error).message);
      // ë‹¤ìŒ fallback ëª¨ë¸ ì‹œë„
    }
  }

  throw lastError || new Error('All models failed');
}

/**
 * Agent Loop ì‹¤í–‰
 */
export async function runAgentLoop(
  requestId: string,
  spaceId: string,
  type: 'INPUT' | 'SEARCH' | 'REFACTOR',
  userInput: string
): Promise<AgentResult> {
  // ìš”ì²­ ë° ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
  const request = await prisma.request.findUnique({
    where: { id: requestId },
    include: {
      user: { select: { loginid: true, username: true, deptname: true } },
    },
  });

  if (!request) throw new Error('Request not found');

  // ê³µê°„ íŠ¸ë¦¬ êµ¬ì¡° ì¡°íšŒ
  const treeStructure = await getTreeStructure(spaceId);

  // ì„¸ì…˜ ì´ˆê¸°í™” (Redisì—ì„œ ì„¤ì •ëœ ëª¨ë¸ ì‚¬ìš©)
  const modelConfig = await getModelConfig();
  const session = createAgentSession(modelConfig.defaultModel);

  // ì´ˆê¸° ë©”ì‹œì§€
  const messages: LLMMessage[] = [
    { role: 'system', content: getSystemPrompt(type, treeStructure) },
    { role: 'user', content: userInput },
  ];

  // ê²°ê³¼ ì¶”ì 
  const result: AgentResult = {
    filesCreated: [],
    filesModified: [],
    foldersCreated: [],
  };

  let iteration = 0;

  while (iteration < MAX_ITERATIONS) {
    iteration++;

    console.log(`[Agent] Iteration ${iteration} for request ${requestId}`);

    // ì§„í–‰ ìƒíƒœ WebSocket ì „ì†¡
    emitRequestProgress(io, requestId, {
      iteration,
      progress: Math.min(iteration / MAX_ITERATIONS * 100, 99),
      message: `ì²˜ë¦¬ ì¤‘... (${iteration}íšŒ)`,
    });

    try {
      // LLM í˜¸ì¶œ
      const response = await callLLM(messages, request.user);

      // í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
      const tokenStatus = updateTokenUsage(session, response.usage);

      console.log(`[Agent] Token usage: ${tokenStatus.usagePercent}% (${tokenStatus.currentPromptTokens} prompt + ${tokenStatus.completionTokens} completion)`);

      // ìš”ì²­ ì—…ë°ì´íŠ¸
      await prisma.request.update({
        where: { id: requestId },
        data: {
          iterations: iteration,
          tokensUsed: response.usage.total_tokens,
        },
      });

      // 80% í† í° ë„ë‹¬ ì‹œ ê²½ê³  ì¶”ê°€
      if (tokenStatus.needsFinish && !messages[0].content?.includes('TOKEN LIMIT WARNING')) {
        messages[0].content += '\n\n' + getTokenWarning(tokenStatus);
      }

      // 100% í† í° ì´ˆê³¼ ì‹œ ê°•ì œ ì¢…ë£Œ
      if (tokenStatus.isExceeded) {
        console.log(`[Agent] Token limit exceeded for request ${requestId}`);

        await sendFailureEmail(
          request.user.loginid,
          request.user.username,
          'í† í° í•œë„ ì´ˆê³¼',
          `ìš”ì²­ ì²˜ë¦¬ ì¤‘ í† í° í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ì„ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`
        );

        throw new Error('Token limit exceeded');
      }

      const choice = response.choices[0];

      if (!choice) {
        throw new Error('No response from LLM');
      }

      const assistantMessage = choice.message;

      // Assistant ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
      // contentëŠ” nullì´ë¼ë„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•¨ (litellm 422 ë°©ì§€)
      messages.push({
        role: 'assistant',
        content: assistantMessage.content || null,
        tool_calls: assistantMessage.tool_calls,
      });

      // Tool call ì²˜ë¦¬
      if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
        for (const toolCall of assistantMessage.tool_calls) {
          const toolName = toolCall.function.name;
          let toolArgs: Record<string, any>;

          try {
            toolArgs = JSON.parse(toolCall.function.arguments);
          } catch {
            toolArgs = {};
          }

          console.log(`[Agent] Tool call: ${toolName}`, toolArgs);

          // complete() í˜¸ì¶œ ì‹œ ì¢…ë£Œ
          if (toolName === 'complete') {
            result.summary = toolArgs.summary;
            if (toolArgs.searchResults) {
              result.searchResults = toolArgs.searchResults;
            }

            // ë¡œê·¸ ê¸°ë¡
            await prisma.requestLog.create({
              data: {
                requestId,
                iteration,
                tool: toolName,
                params: JSON.stringify(toolArgs),
                result: 'completed',
                success: true,
              },
            });

            return result;
          }

          // ë„êµ¬ ì‹¤í–‰
          const startTime = Date.now();
          const toolResult = await executeTool(spaceId, toolName, toolArgs, request.user.loginid);
          const duration = Date.now() - startTime;

          // ê²°ê³¼ ì¶”ì 
          if (toolResult.success) {
            if (toolName === 'add_file') {
              result.filesCreated.push(toolArgs.path);
            } else if (toolName === 'edit_file') {
              result.filesModified.push(toolArgs.path);
            } else if (toolName === 'add_folder') {
              result.foldersCreated.push(toolArgs.path);
            }
          }

          // ë¡œê·¸ ê¸°ë¡
          await prisma.requestLog.create({
            data: {
              requestId,
              iteration,
              tool: toolName,
              params: JSON.stringify(toolArgs),
              result: JSON.stringify(toolResult),
              success: toolResult.success,
              duration,
            },
          });

          // Tool ì‘ë‹µ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            name: toolName,
            content: JSON.stringify(toolResult),
          });
        }
      } else if (choice.finish_reason === 'stop') {
        // Tool call ì—†ì´ ì¢…ë£Œëœ ê²½ìš°
        console.log(`[Agent] LLM finished without complete() call`);
        result.summary = assistantMessage.content || 'Task completed';
        return result;
      }

    } catch (error) {
      console.error(`[Agent] Iteration ${iteration} error:`, error);

      // ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ í™•ì¸
      if (iteration < 3) {
        // ì²˜ìŒ 3ë²ˆì€ ì¬ì‹œë„
        await new Promise(resolve => setTimeout(resolve, 1000 * iteration));
        continue;
      }

      throw error;
    }
  }

  // 100íšŒ ì´ˆê³¼
  console.log(`[Agent] Max iterations reached for request ${requestId}`);

  await sendFailureEmail(
    request.user.loginid,
    request.user.username,
    'ì²˜ë¦¬ íšŸìˆ˜ ì´ˆê³¼',
    `ìš”ì²­ ì²˜ë¦¬ê°€ ${MAX_ITERATIONS}íšŒë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ì„ ê°„ê²°í•˜ê²Œ í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`
  );

  throw new Error(`Max iterations (${MAX_ITERATIONS}) reached`);
}

/**
 * ê³µê°„ì˜ íŠ¸ë¦¬ êµ¬ì¡° ë¬¸ìì—´ ìƒì„±
 */
async function getTreeStructure(spaceId: string): Promise<string> {
  const folders = await prisma.folder.findMany({
    where: { spaceId },
    orderBy: { path: 'asc' },
    select: { path: true },
  });

  const files = await prisma.file.findMany({
    where: { spaceId, deletedAt: null },
    orderBy: { path: 'asc' },
    select: { path: true, name: true },
  });

  const lines: string[] = [];

  for (const folder of folders) {
    lines.push(`ğŸ“ ${folder.path}`);
  }

  for (const file of files) {
    lines.push(`  ğŸ“„ ${file.path}`);
  }

  return lines.join('\n') || '(ë¹ˆ ê³µê°„ - ì•„ì§ ë…¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤)';
}
